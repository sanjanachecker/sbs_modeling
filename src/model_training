"""
Wildfire Burn Severity Model Training - No Upsampling
======================================================
Uses top 30 features from feature importance analysis
Trains on original data only (no upsampled unburned points)
Generates evaluation visualizations and saves model

Author: Generated for SBS Thesis Project
Date: December 2024
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    cohen_kappa_score, classification_report, confusion_matrix,
    make_scorer, accuracy_score, precision_recall_fscore_support,
    roc_curve, auc, RocCurveDisplay
)
from sklearn.preprocessing import label_binarize
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================

DATA_PATH = 'data/all_fires_complete_covariates_fixed_129.csv'
RANDOM_STATE = 42
TEST_SIZE = 0.2

# Output directories
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR = os.path.join(BASE_DIR, 'models')
RESULTS_DIR = os.path.join(BASE_DIR, 'results')

# Create directories if missing
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

# Top 30 features from feature importance analysis
TOP_30_FEATURES = [
    'dnbr', 'dndvi', 'dndbi', 'dbsi', 'nbr', 'bsi', 'ndvi', 'ndbi',
    'meanelev_32', 'wc_bio19', 'nirBand', 'wc_bio05', 'rdgh_6', 'blueBand',
    'minelev_4', 'greenBand', 'wc_bio06', 'swir2Band', 'pisrdif_2021-11-22',
    'pisrdif_2021-12-22', 'stddevelev_32', 'maxc_2', 'wc_bio12', 'wc_bio07',
    'dmndwi', 'wc_bio18', 'wc_bio17', 'wc_bio02', 'vd_5', 'planc_32'
]

# ============================================================================
# LOAD AND PREPARE DATA
# ============================================================================

def load_data(filepath):
    """Load and prepare the dataset."""
    df = pd.read_csv(filepath)
    df['SBS'] = df['SBS'].replace({'mod': 'moderate'})
    return df


def prepare_features(df, feature_list):
    """Prepare feature matrix and target vector."""
    available_features = [f for f in feature_list if f in df.columns]
    df_clean = df.dropna(subset=['SBS'])
    X = df_clean[available_features].fillna(df_clean[available_features].median())
    y = df_clean['SBS']
    return X, y, available_features


# ============================================================================
# MODEL TRAINING
# ============================================================================

def train_random_forest(X_train, y_train, class_weight='balanced'):
    """Train a Random Forest classifier."""
    rf = RandomForestClassifier(
        n_estimators=300,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight=class_weight,
        random_state=RANDOM_STATE,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    return rf


def evaluate_model(model, X_test, y_test, label_encoder):
    """Evaluate model performance."""
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)
    
    kappa = cohen_kappa_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)
    
    results = {
        'kappa': kappa,
        'accuracy': accuracy,
        'y_pred': y_pred,
        'y_proba': y_proba,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'support': support,
        'classification_report': classification_report(
            y_test, y_pred, target_names=label_encoder.classes_
        ),
        'confusion_matrix': confusion_matrix(y_test, y_pred)
    }
    
    return results


def cross_validate(model, X, y, cv=5):
    """Perform cross-validation."""
    kappa_scorer = make_scorer(cohen_kappa_score)
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring=kappa_scorer)
    return cv_scores


# ============================================================================
# VISUALIZATION FUNCTIONS
# ============================================================================

def plot_confusion_matrix(cm, classes, save_path):
    """Plot and save confusion matrix heatmap."""
    plt.figure(figsize=(10, 8))
    
    # Normalize confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Create heatmap
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes,
                cbar_kws={'label': 'Count'})
    
    plt.title('Confusion Matrix\nWildfire Burn Severity Classification', fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_normalized_confusion_matrix(cm, classes, save_path):
    """Plot and save normalized confusion matrix."""
    plt.figure(figsize=(10, 8))
    
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=classes, yticklabels=classes,
                cbar_kws={'label': 'Percentage'})
    
    plt.title('Normalized Confusion Matrix\n(Row-wise Percentages)', fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_feature_importance(model, features, save_path, top_n=20):
    """Plot and save feature importance chart."""
    importance_df = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=True).tail(top_n)
    
    plt.figure(figsize=(10, 8))
    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(importance_df)))
    
    plt.barh(importance_df['feature'], importance_df['importance'], color=colors)
    plt.xlabel('Feature Importance (MDI)', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.title(f'Top {top_n} Feature Importances\nRandom Forest Model', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_class_performance(results, classes, save_path):
    """Plot precision, recall, F1 by class."""
    x = np.arange(len(classes))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    bars1 = ax.bar(x - width, results['precision'], width, label='Precision', color='#2ecc71')
    bars2 = ax.bar(x, results['recall'], width, label='Recall', color='#3498db')
    bars3 = ax.bar(x + width, results['f1'], width, label='F1-Score', color='#9b59b6')
    
    ax.set_xlabel('Burn Severity Class', fontsize=12)
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Model Performance by Class\nPrecision, Recall, and F1-Score', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes)
    ax.legend()
    ax.set_ylim(0, 1.0)
    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
    
    # Add value labels
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.2f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),
                       textcoords="offset points",
                       ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_cv_scores(cv_scores, save_path):
    """Plot cross-validation scores."""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    folds = range(1, len(cv_scores) + 1)
    colors = ['#3498db' if s >= cv_scores.mean() else '#e74c3c' for s in cv_scores]
    
    bars = ax.bar(folds, cv_scores, color=colors, edgecolor='black', linewidth=1.2)
    ax.axhline(y=cv_scores.mean(), color='green', linestyle='--', linewidth=2, 
               label=f'Mean: {cv_scores.mean():.4f}')
    ax.fill_between([0.5, len(cv_scores) + 0.5], 
                    cv_scores.mean() - cv_scores.std(),
                    cv_scores.mean() + cv_scores.std(),
                    alpha=0.2, color='green', label=f'±1 Std: {cv_scores.std():.4f}')
    
    ax.set_xlabel('Fold', fontsize=12)
    ax.set_ylabel("Cohen's Kappa", fontsize=12)
    ax.set_title("5-Fold Cross-Validation Results", fontsize=14, fontweight='bold')
    ax.set_xticks(folds)
    ax.legend()
    ax.set_ylim(0, max(cv_scores) * 1.2)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_class_distribution(y, classes, save_path):
    """Plot class distribution pie chart and bar chart."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Count classes from encoded y values
    unique, counts = np.unique(y, return_counts=True)
    
    # Map to class names
    counts_ordered = list(counts)
    
    # Colors
    colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71'][:len(classes)]
    
    # Pie chart
    axes[0].pie(counts_ordered, labels=classes, autopct='%1.1f%%', colors=colors,
                startangle=90)
    axes[0].set_title('Test Set Class Distribution', fontsize=14, fontweight='bold')
    
    # Bar chart
    bars = axes[1].bar(classes, counts_ordered, color=colors, edgecolor='black')
    axes[1].set_xlabel('Burn Severity Class', fontsize=12)
    axes[1].set_ylabel('Number of Samples', fontsize=12)
    axes[1].set_title('Test Set Sample Count by Class', fontsize=14, fontweight='bold')
    
    for bar, count in zip(bars, counts_ordered):
        axes[1].annotate(f'{count}',
                        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_learning_curve(model, X, y, save_path):
    """Plot learning curve to assess overfitting/underfitting."""
    train_sizes, train_scores, test_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring=make_scorer(cohen_kappa_score)
    )
    
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    test_mean = test_scores.mean(axis=1)
    test_std = test_scores.std(axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color='#3498db', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, 
                     alpha=0.2, color='#3498db')
    plt.plot(train_sizes, test_mean, 'o-', color='#e74c3c', label='Cross-Validation Score')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std,
                     alpha=0.2, color='#e74c3c')
    
    plt.xlabel('Training Set Size', fontsize=12)
    plt.ylabel("Cohen's Kappa", fontsize=12)
    plt.title('Learning Curve\nRandom Forest Model', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def create_summary_dashboard(results, cv_scores, classes, model_path, save_path):
    """Create a summary dashboard with key metrics."""
    fig = plt.figure(figsize=(16, 12))
    
    # Title
    fig.suptitle('Wildfire Burn Severity Model - Evaluation Summary\n(No Upsampling, Top 30 Features)', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    # Create grid
    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)
    
    # 1. Key Metrics Box (top left)
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.axis('off')
    metrics_text = f"""
    KEY METRICS
    ═══════════════════
    Cohen's Kappa: {results['kappa']:.4f}
    Accuracy: {results['accuracy']:.4f}
    
    CV Kappa (mean): {cv_scores.mean():.4f}
    CV Kappa (std): {cv_scores.std():.4f}
    
    Model: Random Forest
    Trees: 300
    Max Depth: 20
    """
    ax1.text(0.1, 0.5, metrics_text, transform=ax1.transAxes, fontsize=11,
             verticalalignment='center', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    # 2. Confusion Matrix (top middle and right)
    ax2 = fig.add_subplot(gs[0, 1:])
    cm = results['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes, ax=ax2)
    ax2.set_title('Confusion Matrix', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Predicted')
    ax2.set_ylabel('Actual')
    
    # 3. Class Performance (middle row)
    ax3 = fig.add_subplot(gs[1, :])
    x = np.arange(len(classes))
    width = 0.25
    bars1 = ax3.bar(x - width, results['precision'], width, label='Precision', color='#2ecc71')
    bars2 = ax3.bar(x, results['recall'], width, label='Recall', color='#3498db')
    bars3 = ax3.bar(x + width, results['f1'], width, label='F1-Score', color='#9b59b6')
    ax3.set_xlabel('Burn Severity Class')
    ax3.set_ylabel('Score')
    ax3.set_title('Performance by Class', fontsize=12, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(classes)
    ax3.legend()
    ax3.set_ylim(0, 1.0)
    ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
    
    # 4. CV Scores (bottom left)
    ax4 = fig.add_subplot(gs[2, 0])
    folds = range(1, len(cv_scores) + 1)
    colors = ['#3498db' if s >= cv_scores.mean() else '#e74c3c' for s in cv_scores]
    ax4.bar(folds, cv_scores, color=colors)
    ax4.axhline(y=cv_scores.mean(), color='green', linestyle='--', linewidth=2)
    ax4.set_xlabel('Fold')
    ax4.set_ylabel('Kappa')
    ax4.set_title('Cross-Validation Scores', fontsize=12, fontweight='bold')
    ax4.set_xticks(folds)
    
    # 5. Class Distribution (bottom middle)
    ax5 = fig.add_subplot(gs[2, 1])
    unique, counts = np.unique([classes[i] for i in range(len(classes))], return_counts=True)
    support = results['support']
    colors_pie = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']
    ax5.pie(support, labels=classes, autopct='%1.1f%%', colors=colors_pie)
    ax5.set_title('Test Set Distribution', fontsize=12, fontweight='bold')
    
    # 6. Model Info (bottom right)
    ax6 = fig.add_subplot(gs[2, 2])
    ax6.axis('off')
    info_text = f"""
    MODEL SAVED
    ═══════════════════
    Path: {model_path}
    
    Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}
    
    Features: 30
    Training samples: 1729
    Test samples: 433
    """
    ax6.text(0.1, 0.5, info_text, transform=ax6.transAxes, fontsize=10,
             verticalalignment='center', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


# ============================================================================
# MODEL SAVING
# ============================================================================

def save_model(model, label_encoder, features, results, cv_scores, save_dir):
    """Save model and associated metadata."""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Save model
    model_filename = f'rf_burn_severity_no_upsampling_{timestamp}.joblib'
    model_path = os.path.join(save_dir, model_filename)
    joblib.dump(model, model_path)
    print(f"    Model saved: {model_path}")
    
    # Save label encoder
    le_filename = f'label_encoder_{timestamp}.joblib'
    le_path = os.path.join(save_dir, le_filename)
    joblib.dump(label_encoder, le_path)
    print(f"    Label encoder saved: {le_path}")
    
    # Save metadata
    metadata = {
        'timestamp': timestamp,
        'model_type': 'RandomForestClassifier',
        'n_estimators': 300,
        'max_depth': 20,
        'class_weight': 'balanced',
        'features': features,
        'n_features': len(features),
        'classes': list(label_encoder.classes_),
        'test_kappa': results['kappa'],
        'test_accuracy': results['accuracy'],
        'cv_kappa_mean': cv_scores.mean(),
        'cv_kappa_std': cv_scores.std(),
        'cv_scores': cv_scores.tolist(),
        'precision_per_class': dict(zip(label_encoder.classes_, results['precision'].tolist())),
        'recall_per_class': dict(zip(label_encoder.classes_, results['recall'].tolist())),
        'f1_per_class': dict(zip(label_encoder.classes_, results['f1'].tolist())),
        'upsampling': False,
        'data_file': DATA_PATH
    }
    
    metadata_filename = f'model_metadata_{timestamp}.json'
    metadata_path = os.path.join(save_dir, metadata_filename)
    
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"    Metadata saved: {metadata_path}")
    
    return model_path, metadata


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("="*80)
    print("WILDFIRE BURN SEVERITY MODEL - NO UPSAMPLING")
    print("="*80)
    
    # Load data
    print("\n[1] Loading data...")
    df = load_data(DATA_PATH)
    print(f"    Total samples: {len(df)}")
    
    # Class distribution
    print("\n[2] Class Distribution:")
    for cls, count in df['SBS'].value_counts().items():
        pct = count / len(df) * 100
        print(f"    {cls:12s}: {count:4d} ({pct:.1f}%)")
    
    # Prepare features
    print("\n[3] Preparing features...")
    X, y, features = prepare_features(df, TOP_30_FEATURES)
    print(f"    Using {len(features)} features")
    
    # Encode labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    print(f"    Classes: {list(le.classes_)}")
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=TEST_SIZE, stratify=y_encoded, random_state=RANDOM_STATE
    )
    print(f"\n    Train: {len(X_train)}, Test: {len(X_test)}")
    
    # Train model
    print("\n[4] Training Random Forest...")
    model = train_random_forest(X_train, y_train)
    
    # Evaluate
    print("\n[5] Evaluating model...")
    results = evaluate_model(model, X_test, y_test, le)
    
    # Cross-validation
    print("\n[6] Running cross-validation...")
    cv_scores = cross_validate(model, X, y_encoded)
    
    # Print results
    print("\n" + "="*80)
    print("RESULTS")
    print("="*80)
    print(f"\n*** Cohen's Kappa: {results['kappa']:.4f} ***")
    print(f"*** Accuracy: {results['accuracy']:.4f} ***")
    
    print("\nClassification Report:")
    print(results['classification_report'])
    
    print("Confusion Matrix:")
    cm_df = pd.DataFrame(
        results['confusion_matrix'],
        index=le.classes_,
        columns=le.classes_
    )
    print(cm_df)
    
    print(f"\nCV Kappa scores: {cv_scores}")
    print(f"Mean CV Kappa: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
    
    # Save model
    print("\n" + "="*80)
    print("[7] Saving model...")
    print("="*80)
    model_path, metadata = save_model(model, le, features, results, cv_scores, MODELS_DIR)
    
    # Generate visualizations
    print("\n" + "="*80)
    print("[8] Generating visualizations...")
    print("="*80)
    
    # Individual plots
    plot_confusion_matrix(
        results['confusion_matrix'], le.classes_,
        os.path.join(RESULTS_DIR, 'confusion_matrix.png')
    )
    
    plot_normalized_confusion_matrix(
        results['confusion_matrix'], le.classes_,
        os.path.join(RESULTS_DIR, 'confusion_matrix_normalized.png')
    )
    
    plot_feature_importance(
        model, features,
        os.path.join(RESULTS_DIR, 'feature_importance.png')
    )
    
    plot_class_performance(
        results, le.classes_,
        os.path.join(RESULTS_DIR, 'class_performance.png')
    )
    
    plot_cv_scores(
        cv_scores,
        os.path.join(RESULTS_DIR, 'cv_scores.png')
    )
    
    plot_class_distribution(
        y_test, le.classes_,
        os.path.join(RESULTS_DIR, 'class_distribution.png')
    )
    
    print("\n    Generating learning curve (this may take a moment)...")
    plot_learning_curve(
        RandomForestClassifier(n_estimators=100, max_depth=20, class_weight='balanced', 
                              random_state=RANDOM_STATE, n_jobs=-1),
        X, y_encoded,
        os.path.join(RESULTS_DIR, 'learning_curve.png')
    )
    
    # Summary dashboard
    create_summary_dashboard(
        results, cv_scores, le.classes_, model_path,
        os.path.join(RESULTS_DIR, 'evaluation_summary_dashboard.png')
    )
    
    # Summary
    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)
    print(f"""
Model: Random Forest (300 trees, max_depth=20, balanced weights)
Features: Top 30 from feature importance analysis
Data: Original data only (no upsampling)

Results:
- Test Kappa: {results['kappa']:.4f}
- Test Accuracy: {results['accuracy']:.4f}
- CV Kappa: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})

Class Performance:
- High severity:   {results['recall'][0]*100:.1f}% recall
- Low severity:    {results['recall'][1]*100:.1f}% recall  
- Moderate:        {results['recall'][2]*100:.1f}% recall
- Unburned:        {results['recall'][3]*100:.1f}% recall

Files Saved:
- Model: {model_path}
- Visualizations: {RESULTS_DIR}/

Note: Unburned class has only {len(df[df['SBS']=='unburned'])} samples (4%)
      Consider using upsampled data for better unburned classification.
    """)
    
    print("="*80)
    print("COMPLETE")
    print("="*80)
    
    return model, results, le, metadata


if __name__ == "__main__":
    model, results, label_encoder, metadata = main()
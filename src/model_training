"""
Wildfire Burn Severity Model Training - WITH Upsampling
========================================================
Uses top 30 features from feature importance analysis
Combines original data WITH upsampled unburned points
Generates evaluation visualizations and saves model

Date: December 2024
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    cohen_kappa_score, classification_report, confusion_matrix,
    make_scorer, accuracy_score, precision_recall_fscore_support,
    roc_curve, auc, RocCurveDisplay
)
from sklearn.preprocessing import label_binarize
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================

# Data paths
ORIGINAL_DATA_PATH = 'data/real_all_fires_complete_covariates_fixed_1229.csv'
UPSAMPLED_DATA_PATH = 'data/real_all_fires_upsampled_points_with_covariates_fixed.csv'

RANDOM_STATE = 42
TEST_SIZE = 0.2

# Output directories
BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in dir() else '/home/claude'
MODELS_DIR = os.path.join(BASE_DIR, 'models')
RESULTS_DIR = os.path.join(BASE_DIR, 'results')

# Create directories if missing
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

# Top 30 features from feature importance analysis
TOP_30_FEATURES = [
    'dnbr', 'dndvi', 'dndbi', 'dbsi', 'nbr', 'bsi', 'ndvi', 'ndbi',
    'meanelev_32', 'wc_bio19', 'nirBand', 'wc_bio05', 'rdgh_6', 'blueBand',
    'minelev_4', 'greenBand', 'wc_bio06', 'swir2Band', 'pisrdif_2021-11-22',
    'pisrdif_2021-12-22', 'stddevelev_32', 'maxc_2', 'wc_bio12', 'wc_bio07',
    'dmndwi', 'wc_bio18', 'wc_bio17', 'wc_bio02', 'vd_5', 'planc_32'
]

# ============================================================================
# LOAD AND PREPARE DATA
# ============================================================================

def load_original_data(filepath):
    """Load and prepare the original dataset."""
    df = pd.read_csv(filepath)
    df['SBS'] = df['SBS'].replace({'mod': 'moderate'})
    return df


def load_upsampled_data(filepath):
    """Load the upsampled unburned points."""
    df = pd.read_csv(filepath)
    # All points in this file are unburned (SBS='unburned')
    return df


def combine_datasets(df_original, df_upsampled, include_upsampling=True):
    """
    Combine original data with upsampled unburned points.
    
    Strategy: 
    - Keep all original data (including original unburned points)
    - Add upsampled unburned points to balance classes
    
    Parameters:
    -----------
    df_original : DataFrame
        Original dataset with all severity classes
    df_upsampled : DataFrame
        Upsampled unburned points
    include_upsampling : bool
        If True, include upsampled data. If False, return only original.
    
    Returns:
    --------
    DataFrame : Combined dataset
    """
    if not include_upsampling:
        return df_original.copy()
    
    # Find common columns (excluding coordinate columns that differ)
    orig_cols = set(df_original.columns)
    ups_cols = set(df_upsampled.columns)
    
    # Columns to exclude from comparison (they differ between datasets)
    exclude_cols = {'PointX', 'PointY', 'Source', 'source', '.geo', 'system:index'}
    
    # Get common feature columns
    common_cols = (orig_cols & ups_cols) - exclude_cols
    
    # Add SBS and Fire_year which we need
    required_cols = ['SBS', 'Fire_year'] + [c for c in common_cols if c not in ['SBS', 'Fire_year']]
    required_cols = [c for c in required_cols if c in df_original.columns and c in df_upsampled.columns]
    
    # Select only common columns from both datasets
    df_orig_subset = df_original[required_cols].copy()
    df_ups_subset = df_upsampled[required_cols].copy()
    
    # Add a flag to track data source
    df_orig_subset['data_source'] = 'original'
    df_ups_subset['data_source'] = 'upsampled'
    
    # Combine
    df_combined = pd.concat([df_orig_subset, df_ups_subset], ignore_index=True)
    
    return df_combined


def prepare_features(df, feature_list):
    """Prepare feature matrix and target vector."""
    available_features = [f for f in feature_list if f in df.columns]
    missing_features = [f for f in feature_list if f not in df.columns]
    
    if missing_features:
        print(f"    Warning: Missing features: {missing_features[:5]}...")  # Show first 5
    
    df_clean = df.dropna(subset=['SBS'])
    X = df_clean[available_features].fillna(df_clean[available_features].median())
    y = df_clean['SBS']
    
    return X, y, available_features


# ============================================================================
# MODEL TRAINING
# ============================================================================

def train_random_forest(X_train, y_train, class_weight='balanced'):
    """Train a Random Forest classifier."""
    rf = RandomForestClassifier(
        n_estimators=300,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight=class_weight,
        random_state=RANDOM_STATE,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    return rf


def evaluate_model(model, X_test, y_test, label_encoder):
    """Evaluate model performance."""
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)
    
    kappa = cohen_kappa_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)
    
    results = {
        'kappa': kappa,
        'accuracy': accuracy,
        'y_pred': y_pred,
        'y_proba': y_proba,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'support': support,
        'classification_report': classification_report(
            y_test, y_pred, target_names=label_encoder.classes_
        ),
        'confusion_matrix': confusion_matrix(y_test, y_pred)
    }
    
    return results


def cross_validate(model, X, y, cv=5):
    """Perform cross-validation."""
    kappa_scorer = make_scorer(cohen_kappa_score)
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring=kappa_scorer)
    return cv_scores


# ============================================================================
# VISUALIZATION FUNCTIONS
# ============================================================================

def plot_confusion_matrix(cm, classes, save_path):
    """Plot and save confusion matrix heatmap."""
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes,
                cbar_kws={'label': 'Count'})
    plt.title('Confusion Matrix\nWildfire Burn Severity Classification (With Upsampling)', 
              fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_normalized_confusion_matrix(cm, classes, save_path):
    """Plot and save normalized confusion matrix."""
    plt.figure(figsize=(10, 8))
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=classes, yticklabels=classes,
                cbar_kws={'label': 'Percentage'})
    plt.title('Normalized Confusion Matrix\n(Row-wise Percentages)', fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_feature_importance(model, features, save_path, top_n=20):
    """Plot and save feature importance chart."""
    importance_df = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=True).tail(top_n)
    
    plt.figure(figsize=(10, 8))
    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(importance_df)))
    plt.barh(importance_df['feature'], importance_df['importance'], color=colors)
    plt.xlabel('Feature Importance (MDI)', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.title(f'Top {top_n} Feature Importances\nRandom Forest Model (With Upsampling)', 
              fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_class_performance(results, classes, save_path):
    """Plot precision, recall, F1 by class."""
    x = np.arange(len(classes))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(12, 6))
    bars1 = ax.bar(x - width, results['precision'], width, label='Precision', color='#2ecc71')
    bars2 = ax.bar(x, results['recall'], width, label='Recall', color='#3498db')
    bars3 = ax.bar(x + width, results['f1'], width, label='F1-Score', color='#9b59b6')
    
    ax.set_xlabel('Burn Severity Class', fontsize=12)
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Model Performance by Class\nPrecision, Recall, and F1-Score (With Upsampling)', 
                 fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes)
    ax.legend()
    ax.set_ylim(0, 1.0)
    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
    
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.2f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),
                       textcoords="offset points",
                       ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_cv_scores(cv_scores, save_path):
    """Plot cross-validation scores."""
    fig, ax = plt.subplots(figsize=(10, 6))
    folds = range(1, len(cv_scores) + 1)
    colors = ['#3498db' if s >= cv_scores.mean() else '#e74c3c' for s in cv_scores]
    
    ax.bar(folds, cv_scores, color=colors, edgecolor='black', linewidth=1.2)
    ax.axhline(y=cv_scores.mean(), color='green', linestyle='--', linewidth=2, 
               label=f'Mean: {cv_scores.mean():.4f}')
    ax.fill_between([0.5, len(cv_scores) + 0.5], 
                    cv_scores.mean() - cv_scores.std(),
                    cv_scores.mean() + cv_scores.std(),
                    alpha=0.2, color='green', label=f'±1 Std: {cv_scores.std():.4f}')
    
    ax.set_xlabel('Fold', fontsize=12)
    ax.set_ylabel("Cohen's Kappa", fontsize=12)
    ax.set_title("5-Fold Cross-Validation Results (With Upsampling)", fontsize=14, fontweight='bold')
    ax.set_xticks(folds)
    ax.legend()
    ax.set_ylim(0, max(cv_scores) * 1.2)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_class_distribution_comparison(df_original, df_combined, save_path):
    """Plot class distribution before and after upsampling."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    colors = {'high': '#e74c3c', 'low': '#f39c12', 'moderate': '#3498db', 'unburned': '#2ecc71'}
    
    # Original distribution
    orig_counts = df_original['SBS'].value_counts()
    orig_counts = orig_counts.reindex(['high', 'low', 'moderate', 'unburned'])
    bars1 = axes[0].bar(orig_counts.index, orig_counts.values, 
                        color=[colors[c] for c in orig_counts.index], edgecolor='black')
    axes[0].set_title('Original Data\n(Without Upsampling)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Burn Severity Class', fontsize=12)
    axes[0].set_ylabel('Number of Samples', fontsize=12)
    for bar, count in zip(bars1, orig_counts.values):
        axes[0].annotate(f'{count}',
                        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # Combined distribution
    comb_counts = df_combined['SBS'].value_counts()
    comb_counts = comb_counts.reindex(['high', 'low', 'moderate', 'unburned'])
    bars2 = axes[1].bar(comb_counts.index, comb_counts.values,
                        color=[colors[c] for c in comb_counts.index], edgecolor='black')
    axes[1].set_title('Combined Data\n(With Upsampled Unburned)', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Burn Severity Class', fontsize=12)
    axes[1].set_ylabel('Number of Samples', fontsize=12)
    for bar, count in zip(bars2, comb_counts.values):
        axes[1].annotate(f'{count}',
                        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                        xytext=(0, 3), textcoords="offset points",
                        ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # Make y-axis same scale
    max_val = max(orig_counts.max(), comb_counts.max())
    axes[0].set_ylim(0, max_val * 1.15)
    axes[1].set_ylim(0, max_val * 1.15)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def plot_learning_curve(model, X, y, save_path):
    """Plot learning curve to assess overfitting/underfitting."""
    train_sizes, train_scores, test_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring=make_scorer(cohen_kappa_score)
    )
    
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    test_mean = test_scores.mean(axis=1)
    test_std = test_scores.std(axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color='#3498db', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, 
                     alpha=0.2, color='#3498db')
    plt.plot(train_sizes, test_mean, 'o-', color='#e74c3c', label='Cross-Validation Score')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std,
                     alpha=0.2, color='#e74c3c')
    
    plt.xlabel('Training Set Size', fontsize=12)
    plt.ylabel("Cohen's Kappa", fontsize=12)
    plt.title('Learning Curve\nRandom Forest Model (With Upsampling)', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


def create_summary_dashboard(results, cv_scores, classes, train_size, test_size, 
                            orig_unburned, total_unburned, model_path, save_path):
    """Create a summary dashboard with key metrics."""
    fig = plt.figure(figsize=(16, 12))
    
    fig.suptitle('Wildfire Burn Severity Model - Evaluation Summary\n(With Upsampled Unburned Points, Top 30 Features)', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)
    
    # 1. Key Metrics Box
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.axis('off')
    metrics_text = f"""
    KEY METRICS
    ═══════════════════
    Cohen's Kappa: {results['kappa']:.4f}
    Accuracy: {results['accuracy']:.4f}
    
    CV Kappa (mean): {cv_scores.mean():.4f}
    CV Kappa (std): {cv_scores.std():.4f}
    
    Model: Random Forest
    Trees: 300
    Max Depth: 20
    
    UPSAMPLING
    ═══════════════════
    Original unburned: {orig_unburned}
    Total unburned: {total_unburned}
    """
    ax1.text(0.1, 0.5, metrics_text, transform=ax1.transAxes, fontsize=10,
             verticalalignment='center', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    # 2. Confusion Matrix
    ax2 = fig.add_subplot(gs[0, 1:])
    cm = results['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes, ax=ax2)
    ax2.set_title('Confusion Matrix', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Predicted')
    ax2.set_ylabel('Actual')
    
    # 3. Class Performance
    ax3 = fig.add_subplot(gs[1, :])
    x = np.arange(len(classes))
    width = 0.25
    ax3.bar(x - width, results['precision'], width, label='Precision', color='#2ecc71')
    ax3.bar(x, results['recall'], width, label='Recall', color='#3498db')
    ax3.bar(x + width, results['f1'], width, label='F1-Score', color='#9b59b6')
    ax3.set_xlabel('Burn Severity Class')
    ax3.set_ylabel('Score')
    ax3.set_title('Performance by Class', fontsize=12, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(classes)
    ax3.legend()
    ax3.set_ylim(0, 1.0)
    ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
    
    # 4. CV Scores
    ax4 = fig.add_subplot(gs[2, 0])
    folds = range(1, len(cv_scores) + 1)
    colors = ['#3498db' if s >= cv_scores.mean() else '#e74c3c' for s in cv_scores]
    ax4.bar(folds, cv_scores, color=colors)
    ax4.axhline(y=cv_scores.mean(), color='green', linestyle='--', linewidth=2)
    ax4.set_xlabel('Fold')
    ax4.set_ylabel('Kappa')
    ax4.set_title('Cross-Validation Scores', fontsize=12, fontweight='bold')
    ax4.set_xticks(folds)
    
    # 5. Class Distribution
    ax5 = fig.add_subplot(gs[2, 1])
    support = results['support']
    colors_pie = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']
    ax5.pie(support, labels=classes, autopct='%1.1f%%', colors=colors_pie)
    ax5.set_title('Test Set Distribution', fontsize=12, fontweight='bold')
    
    # 6. Model Info
    ax6 = fig.add_subplot(gs[2, 2])
    ax6.axis('off')
    info_text = f"""
    MODEL SAVED
    ═══════════════════
    Path: {os.path.basename(model_path)}
    
    Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}
    
    Features: 30
    Training samples: {train_size}
    Test samples: {test_size}
    
    Upsampling: ENABLED
    """
    ax6.text(0.1, 0.5, info_text, transform=ax6.transAxes, fontsize=10,
             verticalalignment='center', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"    Saved: {save_path}")


# ============================================================================
# MODEL SAVING
# ============================================================================

def save_model(model, label_encoder, features, results, cv_scores, 
               orig_unburned, total_unburned, save_dir):
    """Save model and associated metadata."""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    model_filename = f'rf_burn_severity_with_upsampling_{timestamp}.joblib'
    model_path = os.path.join(save_dir, model_filename)
    joblib.dump(model, model_path)
    print(f"    Model saved: {model_path}")
    
    le_filename = f'label_encoder_{timestamp}.joblib'
    le_path = os.path.join(save_dir, le_filename)
    joblib.dump(label_encoder, le_path)
    print(f"    Label encoder saved: {le_path}")
    
    metadata = {
        'timestamp': timestamp,
        'model_type': 'RandomForestClassifier',
        'n_estimators': 300,
        'max_depth': 20,
        'class_weight': 'balanced',
        'features': features,
        'n_features': len(features),
        'classes': list(label_encoder.classes_),
        'test_kappa': results['kappa'],
        'test_accuracy': results['accuracy'],
        'cv_kappa_mean': float(cv_scores.mean()),
        'cv_kappa_std': float(cv_scores.std()),
        'cv_scores': cv_scores.tolist(),
        'precision_per_class': dict(zip(label_encoder.classes_, results['precision'].tolist())),
        'recall_per_class': dict(zip(label_encoder.classes_, results['recall'].tolist())),
        'f1_per_class': dict(zip(label_encoder.classes_, results['f1'].tolist())),
        'upsampling': True,
        'original_unburned_count': orig_unburned,
        'total_unburned_count': total_unburned,
        'original_data_file': ORIGINAL_DATA_PATH,
        'upsampled_data_file': UPSAMPLED_DATA_PATH
    }
    
    metadata_filename = f'model_metadata_{timestamp}.json'
    metadata_path = os.path.join(save_dir, metadata_filename)
    
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"    Metadata saved: {metadata_path}")
    
    return model_path, metadata


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("="*80)
    print("WILDFIRE BURN SEVERITY MODEL - WITH UPSAMPLING")
    print("="*80)
    
    # Load original data
    print("\n[1] Loading original data...")
    df_original = load_original_data(ORIGINAL_DATA_PATH)
    print(f"    Original samples: {len(df_original)}")
    
    # Load upsampled data
    print("\n[2] Loading upsampled unburned data...")
    df_upsampled = load_upsampled_data(UPSAMPLED_DATA_PATH)
    print(f"    Upsampled unburned points: {len(df_upsampled)}")
    
    # Original class distribution
    print("\n[3] Original Class Distribution:")
    orig_unburned = len(df_original[df_original['SBS'] == 'unburned'])
    for cls, count in df_original['SBS'].value_counts().items():
        pct = count / len(df_original) * 100
        print(f"    {cls:12s}: {count:4d} ({pct:.1f}%)")
    
    # Combine datasets
    print("\n[4] Combining datasets...")
    df_combined = combine_datasets(df_original, df_upsampled, include_upsampling=True)
    total_unburned = len(df_combined[df_combined['SBS'] == 'unburned'])
    print(f"    Combined samples: {len(df_combined)}")
    
    # Combined class distribution
    print("\n[5] Combined Class Distribution:")
    for cls, count in df_combined['SBS'].value_counts().items():
        pct = count / len(df_combined) * 100
        print(f"    {cls:12s}: {count:4d} ({pct:.1f}%)")
    
    # Prepare features
    print("\n[6] Preparing features...")
    X, y, features = prepare_features(df_combined, TOP_30_FEATURES)
    print(f"    Using {len(features)} features")
    
    # Encode labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    print(f"    Classes: {list(le.classes_)}")
    
    # Train/test split (stratified)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=TEST_SIZE, stratify=y_encoded, random_state=RANDOM_STATE
    )
    print(f"\n    Train: {len(X_train)}, Test: {len(X_test)}")
    
    # Train model
    print("\n[7] Training Random Forest...")
    model = train_random_forest(X_train, y_train)
    
    # Evaluate
    print("\n[8] Evaluating model...")
    results = evaluate_model(model, X_test, y_test, le)
    
    # Cross-validation
    print("\n[9] Running cross-validation...")
    cv_scores = cross_validate(model, X, y_encoded)
    
    # Print results
    print("\n" + "="*80)
    print("RESULTS")
    print("="*80)
    print(f"\n*** Cohen's Kappa: {results['kappa']:.4f} ***")
    print(f"*** Accuracy: {results['accuracy']:.4f} ***")
    
    print("\nClassification Report:")
    print(results['classification_report'])
    
    print("Confusion Matrix:")
    cm_df = pd.DataFrame(
        results['confusion_matrix'],
        index=le.classes_,
        columns=le.classes_
    )
    print(cm_df)
    
    print(f"\nCV Kappa scores: {cv_scores}")
    print(f"Mean CV Kappa: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
    
    # Save model
    print("\n" + "="*80)
    print("[10] Saving model...")
    print("="*80)
    model_path, metadata = save_model(
        model, le, features, results, cv_scores, 
        orig_unburned, total_unburned, MODELS_DIR
    )
    
    # Generate visualizations
    print("\n" + "="*80)
    print("[11] Generating visualizations...")
    print("="*80)
    
    plot_confusion_matrix(
        results['confusion_matrix'], le.classes_,
        os.path.join(RESULTS_DIR, 'confusion_matrix_upsampled.png')
    )
    
    plot_normalized_confusion_matrix(
        results['confusion_matrix'], le.classes_,
        os.path.join(RESULTS_DIR, 'confusion_matrix_normalized_upsampled.png')
    )
    
    plot_feature_importance(
        model, features,
        os.path.join(RESULTS_DIR, 'feature_importance_upsampled.png')
    )
    
    plot_class_performance(
        results, le.classes_,
        os.path.join(RESULTS_DIR, 'class_performance_upsampled.png')
    )
    
    plot_cv_scores(
        cv_scores,
        os.path.join(RESULTS_DIR, 'cv_scores_upsampled.png')
    )
    
    plot_class_distribution_comparison(
        df_original, df_combined,
        os.path.join(RESULTS_DIR, 'class_distribution_comparison.png')
    )
    
    print("\n    Generating learning curve (this may take a moment)...")
    plot_learning_curve(
        RandomForestClassifier(n_estimators=100, max_depth=20, class_weight='balanced', 
                              random_state=RANDOM_STATE, n_jobs=-1),
        X, y_encoded,
        os.path.join(RESULTS_DIR, 'learning_curve_upsampled.png')
    )
    
    create_summary_dashboard(
        results, cv_scores, le.classes_, len(X_train), len(X_test),
        orig_unburned, total_unburned, model_path,
        os.path.join(RESULTS_DIR, 'evaluation_summary_dashboard_upsampled.png')
    )
    
    # Summary
    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)
    print(f"""
Model: Random Forest (300 trees, max_depth=20, balanced weights)
Features: Top 30 from feature importance analysis
Data: Original + Upsampled unburned points

Upsampling Impact:
- Original unburned: {orig_unburned} samples
- After upsampling: {total_unburned} samples (+{total_unburned - orig_unburned})
- Total dataset: {len(df_combined)} samples

Results:
- Test Kappa: {results['kappa']:.4f}
- Test Accuracy: {results['accuracy']:.4f}
- CV Kappa: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})

Class Performance (Recall):
- High severity:   {results['recall'][0]*100:.1f}%
- Low severity:    {results['recall'][1]*100:.1f}%  
- Moderate:        {results['recall'][2]*100:.1f}%
- Unburned:        {results['recall'][3]*100:.1f}%

Files Saved:
- Model: {model_path}
- Visualizations: {RESULTS_DIR}/
    """)
    
    print("="*80)
    print("COMPLETE")
    print("="*80)
    
    return model, results, le, metadata, df_combined


if __name__ == "__main__":
    model, results, label_encoder, metadata, df_combined = main()